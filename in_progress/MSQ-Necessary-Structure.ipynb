{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MSQ: Necessary Structures\n",
    "\n",
    "This notebook outlines and creates the necessary code structures to implement the MSQ.\n",
    "\n",
    "NOTE: this code is written in Python 2.7.x, but all attempts are made to use Python3-compliant syntax. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import relevant libraries\n",
    "from __future__ import division, print_function\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "from scipy.stats.mstats import mquantiles\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is a very compact description of the method. See the authors' paper for a much more in-depth discussion. \n",
    "\n",
    "## Main Method Objects\n",
    "\n",
    "The key idea is that we define two objects: \n",
    "\n",
    "- a set of *quantiles* of some distribution, denote this $\\mathbf{q}$, of size $s$;\n",
    "- a set of *functions of quantiles*, which map a set of quantiles to a vector of reals. denote this $\\Phi$. \n",
    "\n",
    "This skips a lot of details. For example:\n",
    "\n",
    "- the above set of quantiles and function of quantiles should be thought of as being applied to a *vector* of random variables of length $J$. \n",
    "- For each $j$ random variable, the functions of quantiles should produce a vector of length $M$. Thus the total number of elements in the final vector of reals will be $JM$.\n",
    "- Finally, technically the $\\Phi$ vector of functions of quantiles is a cocmposition of two functions, $\\mathbf{g}, \\mathbf{h}$ in the text. I will use $\\Phi$ in the following. \n",
    "\n",
    "That said, I will use the minimal notation needed to describe the process, and use an example to explore and illustate the details of the process. \n",
    "\n",
    "Assume we have two things: \n",
    "\n",
    "1. A set of M vectors of emperical realizations of the DGP we are trying to fit -- i.e. empirical distributions drawn from the true unknown DGP\n",
    "    - call these the \"empirical\" values\n",
    "    - denote empirical quantiles of these data $\\hat{\\mathbf{q}}$\n",
    "    - denote empirical functions of these quantiles $\\hat{\\Phi}_j$\n",
    "2. A parameterized, simulation-based DGP, from which we can simulate draws conditional on a parameter $\\theta$, \n",
    "    - call these the \"theoretical\" values\n",
    "    - denote theoretical quantiles of these data $\\mathbf{q}_{\\theta}$\n",
    "    - denote theoretical functions of these quantiles $\\Phi_{\\theta, j}$\n",
    "\n",
    "\n",
    "We will explore this in more detail in the example below. \n",
    "\n",
    "To fit the theoretical model to the empirical data, choose the parameter $\\theta$ to minimize the following quadratic objective function:\n",
    "\n",
    "$$\n",
    "\\hat{\\theta} = \\underset{\\theta \\in \\Theta}{\\textrm{argmin}} \\; \\left(\\hat{\\mathbf{\\Phi}} -  \\mathbf{\\Phi}_{\\theta}\\right)^{\\textrm{T}} \\mathbf{W}_{\\theta} \\left(\\hat{\\mathbf{\\Phi}} -  \\mathbf{\\Phi}_{\\theta}\\right).\n",
    "$$\n",
    "\n",
    "Here $\\mathbf{W}_{\\theta}$ is a symmetric positive definite weighting matrix.\n",
    "\n",
    "In addition, the bolded $\\Phi$ values are defined as the \"stacked\" vectors, for each of the $J$ random variables:\n",
    "\n",
    "$$\n",
    "\\mathbf{\\Phi} = \\left(\\Phi_{1}^{\\textrm{T}}, \\Phi_{2}^{\\textrm{T}}, ..., \\Phi_{j}^{\\textrm{T}}, ..., \\Phi_{J}^{\\textrm{T}} \\right)^{\\textrm{T}}\n",
    "$$\n",
    "\n",
    "\n",
    "for both $\\hat{\\Phi}$ and $\\Phi_{\\theta}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Illustration By Example\n",
    "\n",
    "We'll use the example of the $\\alpha$-stable distribution to demonstrate the estimation method.\n",
    "\n",
    "As discussed in the notebook \"An Aside on Alpha Stable Distributions,\" the $\\alpha$-stable distribution is a four-parameter distribution denoted $S(\\alpha, \\beta, \\mu, \\sigma)$ where:\n",
    "\n",
    "- $\\alpha \\in (0,2]$, is the \"tail index:\" it measures the thickness of tails of the distribution. \n",
    "- $\\beta \\in [-1,1]$ is the \"skewness\" parameter. \n",
    "- $\\sigma \\in \\mathbb{R}^{+}$ is the \"scale\" or \"dispersion\" parameter\n",
    "- $\\mu \\in \\mathbb{R}$ is the location parameter\n",
    "\n",
    "There are three special cases of the family of stable distributions:\n",
    "\n",
    "- Normal: $S(\\alpha=2, \\beta=NA, \\frac{\\sigma}{\\sqrt{2}}, \\mu) \\rightarrow \\mathscr{N}(\\mu, \\sigma^2)$\n",
    "- Cauchy: $S(\\alpha=1, \\beta=0, \\sigma, \\mu)$\n",
    "- Levy: $S(\\alpha=0.5, \\beta=1, \\sigma, \\mu)$\n",
    "\n",
    "\n",
    "Importantly, the $\\alpha$ parameter governs whether moments of the distribution exist. For $X \\sim S(\\alpha, \\beta, \\mu, \\sigma)$:\n",
    "\n",
    "$$\\mathbb{E} \\left[ X^{p}\\right] \\lt \\infty \\; \\forall p \\lt \\alpha .$$\n",
    "\n",
    "\n",
    "We use a quantiles-based estimator defined by McCulloch (1986). \n",
    "\n",
    "Let's define the two functions of parameters: \n",
    "\n",
    "* The theoretical function of parameters, $\\Phi_{\\theta}$: \n",
    "\n",
    "$$\n",
    "\\Phi_{\\theta}=\\left(\n",
    "    \\begin{array}{c}\n",
    "        \\frac{ q_{0.95, \\theta} \\,-\\, q_{0.05, \\theta} } { q_{0.75, \\theta} \\,-\\, q_{0.25, \\theta} } \\\\\n",
    "        \\frac{ (q_{0.95, \\theta} \\,-\\, q_{0.5, \\theta}) \\,+\\, (q_{0.05, \\theta} \\,-\\, q_{0.5, \\theta}) } { q_{0.95, \\theta} \\,-\\, q_{0.05, \\theta} }  \\\\\n",
    "         (q_{0.75, \\theta} \\,-\\, q_{0.25, \\theta}) \\sigma \\\\\n",
    "         q_{0.5, \\theta} \\sigma + \\mu\n",
    "    \\end{array}\n",
    "    \\right)\n",
    "$$\n",
    "\n",
    "\n",
    "* The empirical function of parameters, $\\hat{\\Phi}$: \n",
    "\n",
    "$$\n",
    "\\hat{\\Phi}=\\left(\n",
    "    \\begin{array}{c}\n",
    "        \\frac{ \\hat{q}_{0.95} \\,-\\, \\hat{q}_{0.05} } { \\hat{q}_{0.75} \\,-\\, \\hat{q}_{0.25} } \\\\\n",
    "        \\frac{ (\\hat{q}_{0.95} \\,-\\, \\hat{q}_{0.5}) \\,+\\, (\\hat{q}_{0.05} \\,-\\, \\hat{q}_{0.5}) } { \\hat{q}_{0.95} \\,-\\, \\hat{q}_{0.05} }  \\\\\n",
    "         (\\hat{q}_{0.75} \\,-\\, \\hat{q}_{0.25}) \\\\\n",
    "         \\hat{q}_{0.5}\n",
    "    \\end{array}\n",
    "    \\right)\n",
    "$$\n",
    "\n",
    "\n",
    "Dominicy and Verdas (2013) follow McCulloch (1986) and standardize the empirical data which comprises $\\hat{\\Phi}$, and thus additionally standardize the theoretical measurements. \n",
    "\n",
    "The theoretical function of parameters $\\Phi_{\\theta}$ is obtained by drawing a simulated sample of data from the distribution implied by $\\theta$, which we will denote as $\\overset{\\sim}{\\Phi}_{\\theta}$.\n",
    "\n",
    "Finally, to represent $\\Phi_{\\theta}$ we draw $R$ simulation paths and find the average value: \n",
    "\n",
    "$$\n",
    "\\overset{\\sim}{\\Phi}_{\\theta}^{R} = \\frac{1}{R}\\sum_{r=1}^{R} \\overset{\\sim}{\\Phi}_{\\theta}^{r}.\n",
    "$$\n",
    "\n",
    "We find the $\\hat{\\theta}$ by minimizing the quadratic objective: \n",
    "\n",
    "$$\n",
    "\\hat{\\theta} = \\underset{\\theta \\in \\Theta}{\\textrm{argmin}} \\; \\left(\\hat{\\mathbf{\\Phi}} - \\mathbf{\\overset{\\sim}{\\Phi}}_{\\theta}^{R} \\right)^{\\textrm{T}} \\mathbf{W}_{\\theta} \\left(\\hat{\\mathbf{\\Phi}} -  \\mathbf{\\overset{\\sim}{\\Phi}}_{\\theta}^{R}\\right).\n",
    "$$\n",
    "\n",
    "\n",
    "There are a few more details which we cover as we proceed.\n",
    "\n",
    "\n",
    "## Code Structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's define some functions:\n",
    "\n",
    "\n",
    "def h(q):\n",
    "    # Assume qhat = qhat_{.05}, qhat_{.25}, qhat_{.50}, qhat_{.75}, qhat_{.95} \n",
    "    return np.array([ q[-1] - q[0],\n",
    "                      q[-1] - 2 * q[2] + q[0],\n",
    "                      q[-2] - q[1], \n",
    "                      q[2] ])\n",
    "\n",
    "def g(q):\n",
    "    # Assume qhat = qhat_star_{.05}, qhat_star_{.25}, qhat_star_{.50},  qhat_star_{.75}, qhat_star_{.95} \n",
    "    return np.array([ 1.0 / (q[-2] - q[1]),\n",
    "                      1.0 / (q[-1] - q[0]),\n",
    "                      1.0, \n",
    "                      1.0 ])\n",
    "\n",
    "# Test this:\n",
    "def Phi_hat_alt(q):\n",
    "    return h(q) * g(q)\n",
    "\n",
    "\n",
    "def Phi_hat(q):\n",
    "    '''\n",
    "    q is a vector of quantiles in ascending order. The function will \n",
    "    error-check for this.\n",
    "    '''\n",
    "    # Check if q is in order:\n",
    "    #assert np.all(np.diff(q) >= 0), \"Quantiles q are not in ascending order: q=\"+str(q)\n",
    "    \n",
    "    return np.array([ (q[-1] - q[0]) / (q[-2] - q[1]),\n",
    "                      (q[-1] - 2 * q[2] + q[0]) / (q[-1] - q[0]),\n",
    "                       q[-2] - q[1], \n",
    "                       q[2] ])\n",
    "    \n",
    "def Phi_theta_plain(q, mu, sigma):\n",
    "    '''\n",
    "    q is a vector of quantiles in ascending order. The function will \n",
    "    error-check for this.\n",
    "    mu and sigma are float values for the mu, sigma values in the function. \n",
    "    '''\n",
    "    # Assert that q is in ascending order!\n",
    "    #assert np.all(np.diff(q) >= 0), \"Quantiles q are not in ascending order: q=\"+str(q)\n",
    "    \n",
    "    return np.array( [ (q[-1] - q[0]) / (q[-2] - q[1]),\n",
    "                       (q[-1] - 2 * q[2] + q[0]) / (q[-1] - q[0]),\n",
    "                       (q[-2] - q[1]) * sigma, \n",
    "                        q[2] * sigma + mu] )\n",
    "\n",
    "\n",
    "def Phi_theta(q, theta):\n",
    "    '''\n",
    "    q is a vector of quantiles in ascending order. The function will \n",
    "    error-check for this.\n",
    "    mu and sigma are float values for the mu, sigma values in the function. \n",
    "    '''\n",
    "    # Assert that q is in ascending order!\n",
    "    #assert np.all(np.diff(q) >= 0), \"Quantiles q are not in ascending order: q=\"+str(q)\n",
    "    \n",
    "    # Recall:\n",
    "    #theta = {'alpha':theta_vec[0], \n",
    "    #         'beta':theta_vec[1],\n",
    "    #         'mu':theta_vec[2],\n",
    "    #         'sigma':theta_vec[3]}\n",
    "    \n",
    "    if theta[0] != 1.0:\n",
    "        zeta = theta[2] + theta[1]*theta[3]*np.tan(np.pi * theta[0] / 2.0) \n",
    "        # zeta=      mu +    beta * sigma  *   tan(   pi * alpha    / 2)\n",
    "    else:\n",
    "        zeta = theta[2] # mu\n",
    "    \n",
    "    return np.array( [ (q[-1] - q[0]) / (q[-2] - q[1]),\n",
    "                       (q[-1] - 2 * q[2] + q[0]) / (q[-1] - q[0]),\n",
    "                       (q[-2] - q[1]) * theta[3], \n",
    "                        q[2] * theta[3] + zeta] )\n",
    "\n",
    "\n",
    "\n",
    "def Phi_theta_dict(q, theta):\n",
    "    '''\n",
    "    q is a vector of quantiles in ascending order. The function will \n",
    "    error-check for this.\n",
    "    mu and sigma are float values for the mu, sigma values in the function. \n",
    "    '''\n",
    "    # Assert that q is in ascending order!\n",
    "    #assert np.all(np.diff(q) >= 0), \"Quantiles q are not in ascending order: q=\"+str(q)\n",
    "    \n",
    "    # Recall:\n",
    "    #theta = {'alpha':theta_vec[0], \n",
    "    #         'beta':theta_vec[1],\n",
    "    #         'mu':theta_vec[2],\n",
    "    #         'sigma':theta_vec[3]}\n",
    "    \n",
    "    if theta['alpha'] != 1.0:\n",
    "        zeta = theta['mu'] + theta['beta']*theta['sigma']*np.tan(np.pi * theta['alpha'] / 2.0) \n",
    "        # zeta=      mu +    beta * sigma  *   tan(   pi * alpha    / 2)\n",
    "    else:\n",
    "        zeta = theta['mu'] # mu\n",
    "    \n",
    "    return np.array( [ (q[-1] - q[0]) / (q[-2] - q[1]),\n",
    "                       (q[-1] - 2 * q[2] + q[0]) / (q[-1] - q[0]),\n",
    "                       (q[-2] - q[1]) * theta['sigma'], \n",
    "                        q[2] * theta['sigma'] + zeta] )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Generate data\n",
    "def generate_data(theta, N, R, rng):\n",
    "    '''\n",
    "    The parameter theta contains the alpha-stable distribution parameters.\n",
    "    theta must be a dict with keys: alpha, beta, mu, sigma.\n",
    "    N is length of vector of IID draws, R is number of vector.\n",
    "    rng is a numpy RandomState instance.\n",
    "    seed is for a random number generator.'''\n",
    "    \n",
    "    #assert theta['alpha'] > 0 and theta['alpha'] <= 2, \"alpha not in (0,1]: alpha = \"+str(theta['alpha'])\n",
    "    #assert theta['beta'] >= -1 and theta['beta'] <= 1, \"beta not in [-1,1]: beta = \"+str(theta['beta'])\n",
    "    #assert theta['sigma'] >= 0 , \"sigma not >= 0: sigma = \"+str(theta['sigma'])\n",
    "    \n",
    "    # Generate the data \n",
    "    return stats.levy_stable.rvs(alpha=theta['alpha'], beta=theta['beta'], \n",
    "                                 loc=theta['mu'], scale=theta['sigma'], \n",
    "                                 size=(N, R), random_state=rng)\n",
    "\n",
    "\n",
    "\n",
    "def generate_data_vec(theta, N, R, rng, testing=False):\n",
    "    '''\n",
    "    Same as \"generate_data\" but with theta as an ordered numpy vector.\n",
    "    The parameter theta contains the alpha-stable distribution parameters.\n",
    "    theta values must be in the order: alpha, beta, mu, sigma.\n",
    "    N is length of vector of IID draws, R is number of vector.\n",
    "    rng is a numpy RandomState instance.\n",
    "    seed is for a random number generator.'''\n",
    "    \n",
    "    #if testing:\n",
    "    #    assert theta[0] > 0 and theta[0] <= 2, \"alpha not in (0,1]: alpha = \"+str(theta[0])\n",
    "    #    assert theta[1] >= -1 and theta[1] <= 1, \"beta not in [-1,1]: beta = \"+str(theta[1])\n",
    "    #    assert theta[3] >= 0 , \"sigma not >= 0: sigma = \"+str(theta[3])\n",
    "    \n",
    "    # Generate the data \n",
    "    return stats.levy_stable.rvs(alpha=theta[0], beta=theta[1], \n",
    "                                 loc=theta[2], scale=theta[3], \n",
    "                                 size=(N, R), random_state=rng)\n",
    "\n",
    "\n",
    "def find_emperical_quantiles(data, q):\n",
    "    '''\n",
    "    data is a 1D or 2D vector of data,\n",
    "    q are the quantiles. \n",
    "    '''\n",
    "    # Draw quantiles from the data. NOTE: Assume that data is (N,R)\n",
    "    # shape. Note that returned values will have shape: (len(q), R)\n",
    "    # NOTE2: this is only for exposition; just directly use mquantiles.\n",
    "    return mquantiles(a=data, prob=q, alphap=0.4, betap=0.4, axis=0)\n",
    "\n",
    "def find_theoretical_quantiles_squiggle_R(theta, N, R, q, rng):\n",
    "    '''\n",
    "    Construct the thereotical quantiles by simulation. Given \n",
    "    theta, N, R, q, and rng, return a vector of the five quantiles \n",
    "    associated with this distribution. \n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    theta: dict of stable distribution parameters. Must be a dict \n",
    "           with keys: alpha, beta, mu, sigma\n",
    "    N: int, number of observations per simulation\n",
    "    R: int, number of simulations to run\n",
    "    q: vector of floats; the quantiles values. Assumed in ascending order.\n",
    "    rng: a NumPy RandomState object. Allows reproducibility.\n",
    "\n",
    "    Returns:\n",
    "    -----------\n",
    "    q_hat: vector of floats, the average of quantilels over R simulations\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Assert that q is in ascending order!\n",
    "    #assert np.all(np.diff(q) > 0), \"Quantiles q are not in ascending order: q=\"+str(q)\n",
    "\n",
    "    # Generate data:\n",
    "    # NOTE: data is (N,R) shape:\n",
    "    data  = generate_data(theta, N, R, rng)\n",
    "\n",
    "    # Find the quantiles:\n",
    "    # Draw quantiles from the data. NOTE: Assume that data is (N,R)\n",
    "    # shape. Note that returned values will have shape: (len(q), R)\n",
    "    quantiles_R = mquantiles(a=data, prob=q, alphap=0.4, betap=0.4, axis=0)\n",
    "    \n",
    "    # Average over each quantile; the resulting vector will be\n",
    "    #in ascending order:\n",
    "    return np.apply_along_axis(func1d=np.mean, axis=1, arr=quantiles_R) #theory_quantiles\n",
    "\n",
    "def find_theoretical_quantiles_squiggle_R_vec(theta_vec, N, R, q, rng, testing=False):\n",
    "    '''\n",
    "    Construct the thereotical quantiles by simulation. Given \n",
    "    theta, N, R, q, and rng, return a vector of the five quantiles \n",
    "    associated with this distribution. \n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    theta: vector of stable distribution parameters. Must be a numpy array \n",
    "           with floats in this order: alpha, beta, mu, sigma\n",
    "    N: int, number of observations per simulation\n",
    "    R: int, number of simulations to run\n",
    "    q: vector of floats; the quantiles values. Assumed in ascending order.\n",
    "    rng: a NumPy RandomState object. Allows reproducibility.\n",
    "\n",
    "    Returns:\n",
    "    -----------\n",
    "    q_hat: vector of floats, the average of quantilels over R simulations\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Assert that q is in ascending order!\n",
    "    #if testing:\n",
    "    #    assert np.all(np.diff(q) > 0), \"Quantiles q are not in ascending order: q=\"+str(q)\n",
    "\n",
    "    # Generate data:\n",
    "    # NOTE: data is (N,R) shape:\n",
    "    data  = generate_data_vec(theta_vec, N, R, rng)\n",
    "\n",
    "    # Find the quantiles:\n",
    "    # Draw quantiles from the data. NOTE: Assume that data is (N,R)\n",
    "    # shape. Note that returned values will have shape: (len(q), R)\n",
    "    quantiles_R = mquantiles(a=data, prob=q, alphap=0.4, betap=0.4, axis=0)\n",
    "    \n",
    "    # Average over each quantile; the resulting vector will be\n",
    "    #in ascending order:\n",
    "    return np.apply_along_axis(func1d=np.mean, axis=1, arr=quantiles_R) #theory_quantiles =  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def sparsity_function():\n",
    "    pass\n",
    "\n",
    "def G_hat():\n",
    "    # Diagonal matrix \n",
    "    # Diagonal elements are g( q_hat^{*}_{j}  )\n",
    "    # Full thing:\n",
    "    # \n",
    "    #     g(q_hat^{*}) = ( g(q_hat^{*}_{1})^T, g(q_hat^{*}_2)^T,  ... ,  g(q_hat^{*}_{J})^T   )^T\n",
    "    # \n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.25848274987\n",
      "2.25848274987\n",
      "18.1090826422\n"
     ]
    }
   ],
   "source": [
    "# Next steps: \n",
    "# Generate a sample of data from a \"mystery dist\"\n",
    "# Choose starting theta\n",
    "# run estimation *step 1 only*.\n",
    "\n",
    "q = [0.05, 0.25, 0.5, 0.75, 0.95]\n",
    "\n",
    "mystery_theta = {'alpha':1.5, \n",
    "                 'beta':-0.5,\n",
    "                 'mu':0.5,\n",
    "                 'sigma':1.2}\n",
    "# Cauchy: alpha=1, beta=0, loc=mu=0.0, scale=sigma/2.0 = 1.0/2.0\n",
    "\n",
    "# Using raw data:\n",
    "# array([ 1.33024131, -0.57463142, -0.16851961,  0.96667289])\n",
    "# Using standardized sample, with sigma=std(data):\n",
    "#array([ 1.33023827, -0.57449001,  0.06295755,  0.21148216])\n",
    "'''\n",
    "Optimization terminated successfully.    (Exit mode 0)\n",
    "            Current function value: 3.93901180299e-08\n",
    "            Iterations: 15\n",
    "            Function evaluations: 95\n",
    "            Gradient evaluations: 15\n",
    "Took:7.11473720074 min.\n",
    "Out[7]:\n",
    "     fun: 3.9390118029863406e-08\n",
    "     jac: array([-0.00065797,  0.00016368,  0.00025707,  0.00022284,  0.        ])\n",
    " message: 'Optimization terminated successfully.'\n",
    "    nfev: 95\n",
    "     nit: 15\n",
    "    njev: 15\n",
    "  status: 0\n",
    " success: True\n",
    "       x: array([ 1.33023827, -0.57449001,  0.06295755,  0.21148216])\n",
    "'''\n",
    "# SO NOTE: THIS APPEARS WRONG! using std.\n",
    "# Next:\n",
    "# - try with 'use sigma/mu to stdize sample'\n",
    "# - update W_theta\n",
    "# - ...email\n",
    "\n",
    "\n",
    "# RNG setup:\n",
    "seed0 = 567891234\n",
    "rng = np.random.RandomState(seed0)\n",
    "\n",
    "# Draw sample of mystery data:\n",
    "Msample = 200\n",
    "empirical_data = generate_data(theta=mystery_theta, N=Msample, R=1, rng=rng)\n",
    "\n",
    "#N=10000\n",
    "#R=200\n",
    "\n",
    "N=7500\n",
    "R=150\n",
    "\n",
    "# Standardize the sample?\n",
    "# Need to ask authors about this!\n",
    "z_empirical_data = (empirical_data - np.mean(empirical_data) ) / np.std(empirical_data)\n",
    "\n",
    "# NOTE: confusion over what \"standardize the sample\" means. Here are there options: \n",
    "# 1. don't standardize\n",
    "# 2. standardize using mean, std. \n",
    "# 3. for each theta_check, after generatefs all the samples, \n",
    "#    figure out quantiles, and then a,b,mu, sig, use *that* mu andd sig to normalize. \n",
    "# Still not clear why would do this (3) one. But whatevs. \n",
    "# SO, start and try out the (2) version...yes?\n",
    "\n",
    "\n",
    "# Process:\n",
    "# Form the quantiles over the data \n",
    "# Form W0\n",
    "# Choose a theta0\n",
    "# Generate theta_squiggle_R from theta0\n",
    "# Find the quandratic value\n",
    "# Iterate\n",
    "\n",
    "# Get data quantiles:\n",
    "W0 = np.eye(4)\n",
    "#empirical_q =  mquantiles(a=z_empirical_data, prob=q, alphap=0.4, betap=0.4, axis=0).compressed() # Remove masking\n",
    "empirical_q =  mquantiles(a=empirical_data, prob=q, alphap=0.4, betap=0.4, axis=0).compressed() # Remove masking\n",
    "\n",
    "theta_hat = Phi_hat(empirical_q)\n",
    "\n",
    "theta0 = {'alpha':1.2, \n",
    "          'beta':-0.25,\n",
    "          'mu':0.5,\n",
    "          'sigma':0.5}  # Levy: alpha=0.5, beta=1.0, loc=mu, scale=sigma/2.0\n",
    "# Note: *internal* to the function, unpack these!\n",
    "\n",
    "theta_vec = np.array([theta0['alpha'], theta0['beta'], theta0['mu'], theta0['sigma'], ])\n",
    "\n",
    "def quadratic_objective_old_dict_style(theta, W, theta_hat, seed, N, R, qvals):\n",
    "    #unpack:\n",
    "    #theta = {'alpha':theta_vec[0], \n",
    "    #         'beta':theta_vec[1],\n",
    "    #         'mu':theta_vec[2],\n",
    "    #         'sigma':theta_vec[3]} \n",
    "\n",
    "    # Generate theta_squiggle_R from theta0:\n",
    "    rng = np.random.RandomState(seed)\n",
    "    theta_squiggle_R_q = find_theoretical_quantiles_squiggle_R(theta=theta, N=N, R=R, q=qvals, rng=rng)\n",
    "\n",
    "    theta_squiggle_R = Phi_theta_dict(q=theta_squiggle_R_q, theta=theta) # mu=theta['mu'], sigma=theta['sigma'])\n",
    "\n",
    "    theta_minus_theta = theta_hat - theta_squiggle_R\n",
    "\n",
    "    return np.dot(theta_minus_theta, W0).dot(theta_minus_theta)\n",
    "\n",
    "\n",
    "def quadratic_objective(theta_vec, W, theta_hat, seed, N, R, qvals):\n",
    "    # Generate theta_squiggle_R from theta0:\n",
    "    # NOTE: this takes in the \"theta_hat\" already calculated. Need to think \n",
    "    # about whether to *always* recalcuate the the theta_hat based on the \n",
    "    # empirical data, **standardized by the theoretical vavlues**.\n",
    "    # I *presume* that will be more expensive. \n",
    "    rng = np.random.RandomState(seed)\n",
    "    \n",
    "    theta_squiggle_R_q = find_theoretical_quantiles_squiggle_R_vec(theta_vec=theta_vec, N=N, R=R, q=qvals, rng=rng)\n",
    "\n",
    "    theta_squiggle_R = Phi_theta(q=theta_squiggle_R_q, theta=theta_vec)  # mu=theta_vec[2], sigma=theta_vec[3])\n",
    "\n",
    "    theta_minus_theta = theta_hat - theta_squiggle_R\n",
    "\n",
    "    return np.dot(theta_minus_theta, W0).dot(theta_minus_theta)\n",
    "\n",
    "\n",
    "def quadratic_objective_vec_recalculate_theta_hat(theta_vec, W, seed, N, R, qvals, verbose=False, the_data=empirical_data):\n",
    "    # Generate theta_squiggle_R from theta0:\n",
    "    # This version takes in the empirical data and *always* recalcuates\n",
    "    # the the theta_hat based on the \n",
    "    # empirical data, **standardized by the theoretical vavlues**.\n",
    "    # I *presume* that will be more expensive. \n",
    "    # Note that I am *binding* the empirical data to the function. Will see how that goes...\n",
    "    \n",
    "    # Recall:\n",
    "    #     theta = {'alpha':theta_vec[0], \n",
    "    #         'beta':theta_vec[1],\n",
    "    #         'mu':theta_vec[2],\n",
    "    #         'sigma':theta_vec[3]} \n",
    "    \n",
    "    if verbose:\n",
    "        print(\"mu=\"+str(theta_vec[2]))\n",
    "        print(\"sigma=\"+str(theta_vec[3]))\n",
    "    \n",
    "    # First find theta-hat:\n",
    "    standardized_data = (the_data - theta_vec[2]) / theta_vec[3]\n",
    "\n",
    "    # Now find the quantiles: \n",
    "    empirical_q =  mquantiles(a=standardized_data, prob=q, alphap=0.4, betap=0.4, axis=0).compressed() # Remove masking\n",
    "\n",
    "    # *now* find the theta-hat:\n",
    "    theta_hat = Phi_hat(empirical_q)\n",
    "\n",
    "    # Now rest of the run:\n",
    "    rng = np.random.RandomState(seed)\n",
    "    \n",
    "    theta_squiggle_R_q = find_theoretical_quantiles_squiggle_R_vec(theta_vec=theta_vec, N=N, R=R, q=qvals, rng=rng)\n",
    "\n",
    "    theta_squiggle_R = Phi_theta(q=theta_squiggle_R_q, theta=theta_vec) #mu=theta_vec[2], sigma=theta_vec[3])\n",
    "\n",
    "    theta_minus_theta = theta_hat - theta_squiggle_R\n",
    "\n",
    "    return np.dot(theta_minus_theta, W0).dot(theta_minus_theta)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "seed1=1236789\n",
    "quadratic_objective(theta_vec, W0, theta_hat, seed1, N, R, q)\n",
    "\n",
    "# Let's examine:\n",
    "\n",
    "#theta_vec_true = np.array([theta0['alpha'], theta0['beta'], theta0['mu'], theta0['sigma'], ])\n",
    "\n",
    "print(quadratic_objective(theta_vec, W0, theta_hat, seed1, N, R, q))\n",
    "print(quadratic_objective_old_dict_style(theta0, W0, theta_hat, seed1, N, R, q))\n",
    "\n",
    "\n",
    "print(quadratic_objective_vec_recalculate_theta_hat(theta_vec, W0, seed1, N, R, q))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 8.32978215,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        , -3.06053955,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  2.33366229,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  1.0102306 ]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_hat\n",
    "\n",
    "\n",
    "def G_theta(q):\n",
    "    return np.diag(g(q))\n",
    "\n",
    "def Omega_theta(q):\n",
    "    return np.diag(h(q))\n",
    "\n",
    "def Wstar_theta(q):\n",
    "    pass\n",
    "\n",
    "\n",
    "g(empirical_q)\n",
    "h(empirical_q)\n",
    "\n",
    "#np.diag(g(empirical_q))\n",
    "#np.diag(h(empirical_q))\n",
    "#W0\n",
    "\n",
    "#G_theta(empirical_q)\n",
    "Omega_theta(empirical_q)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Denote by $\\hat{G}_j$ a M x M diagonal matrix with diagonal elements $g(\\hat{q}_{j}^{*})$. \n",
    "\n",
    "We gather all these vectors into \n",
    "$$g( \\hat{q}^{*}) = \\left( g(\\hat{q}_{1}^{*})^{T}, ..., g(\\hat{q}_{J}^{*})^{T} \\right)^{T} $$\n",
    "\n",
    "with the corresponding block diagonal JM x JM matrix \n",
    "\n",
    "$$\\hat{G} = diag(diag(g(\\hat{q}_{1}^{*}), ..., diag(g(\\hat{q}_{J}^{*}))).$$\n",
    "\n",
    "Similarly, let $G_{\\theta}$ be a JM × JM diagonal matrix composed of J diagonal blocks, each of size M :\n",
    "\n",
    "$$G_{\\theta} = diag(diag( g(q_{\\theta,1}^{*} )), . . . , diag( g(q_{\\theta,M}^{*} ))).$$\n",
    "\n",
    "\n",
    "A result: $\\hat{G} \\rightarrow G_{\\theta}$ as ________.\n",
    "\n",
    "**Major Q:** what is the convergence iteration here? How do we go from $\\hat{q}_{k}^{*} \\rightarrow \\hat{q}_{k+1}^{*}?$\n",
    "\n",
    "**Are we doing the following?**\n",
    "\n",
    "- choose W0. \n",
    "- estimate $\\theta$ (and corresponding qs with $\\theta$...\n",
    "- *that's iteration k=1*\n",
    "- **NOW**, use $theta$ to create $W_{theta,1}$\n",
    "- repeat from step 1\n",
    "- decide convergence on what criteria?\n",
    "\n",
    "\n",
    "\n",
    "### Some Qs:\n",
    "\n",
    "- standardizing the sample\n",
    "- construction of $\\Phi_{\\theta}$ in p237, what is role of $\\zeta$?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WHERE AT:\n",
    "\n",
    "Updated the Phi_theta function to include the zeta value. \n",
    "\n",
    "**BUT** getting the  wrong sign on mu. Need to think about -- see if can swtichc back and get \"old\"results.\n",
    "\n",
    "Also need to think more about iteration to $W_{\\theta}$. Need to be careful about what exactly the iteration steps are. \n",
    "\n",
    "Need also to think aa\n",
    "\n",
    "Also need to email Qs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 8.41228190979e-07\n",
      "            Iterations: 15\n",
      "            Function evaluations: 98\n",
      "            Gradient evaluations: 15\n",
      "Took:6.31039623419 min.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "     fun: 8.4122819097921929e-07\n",
       "     jac: array([ 0.0036164 , -0.00094655,  0.00042372,  0.00515214,  0.        ])\n",
       " message: 'Optimization terminated successfully.'\n",
       "    nfev: 98\n",
       "     nit: 15\n",
       "    njev: 15\n",
       "  status: 0\n",
       " success: True\n",
       "       x: array([ 1.33501652, -0.5832553 , -0.31480513,  1.03965954])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# USING THE NEW -ALWAYS-RECALCUALTE VERSION:\n",
    "\n",
    "from time import time\n",
    "t0=time()\n",
    "\n",
    "near_zero = 1.0e-12\n",
    "res = minimize(fun=quadratic_objective_vec_recalculate_theta_hat, x0=theta_vec, args=(W0, seed1, N, R, q), method='SLSQP',\n",
    "               jac=None, bounds=[(near_zero, 2.0), (-1.0+near_zero, 1.0-near_zero), (None, None), (near_zero, None)], options={\"disp\":True} ) \n",
    "\n",
    "t1=time()\n",
    "print(\"Took:\"+str( (t1-t0)/60. )+ \" min.\")\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 8.44617269559e-08\n",
      "            Iterations: 14\n",
      "            Function evaluations: 89\n",
      "            Gradient evaluations: 14\n",
      "Took:5.1443947355 min.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "     fun: 8.4461726955918891e-08\n",
       "     jac: array([  1.72377228e-03,  -1.95518935e-04,  -7.76383442e-05,\n",
       "        -2.46858701e-04,   0.00000000e+00])\n",
       " message: 'Optimization terminated successfully.'\n",
       "    nfev: 89\n",
       "     nit: 14\n",
       "    njev: 14\n",
       "  status: 0\n",
       " success: True\n",
       "       x: array([ 1.33478914, -0.58379854, -0.11270155,  0.21146677])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Usign old version:\n",
    "\n",
    "from time import time\n",
    "t0=time()\n",
    "#quadratic_objective_vec\n",
    "\n",
    "res = minimize(fun=quadratic_objective, x0=theta_vec, args=(W0, theta_hat, seed1, N, R, q), method='SLSQP',\n",
    "               jac=None, bounds=[(near_zero, 2.0), (-1.0+near_zero, 1.0-near_zero), (None, None), (near_zero, None)], options={\"disp\":True} ) \n",
    "\n",
    "#res = minimize(fun=quadratic_objective, x0=theta_vec, args=(W0, theta_hat, seed1, N, R, q), method='SLSQP',\n",
    "#               jac=None, bounds=[(0.51, 2.0), (-0.99999999, 0.9999999), (None, None), (0.0, None)], options={\"disp\":True} ) \n",
    "\n",
    "\n",
    "t1=time()\n",
    "print(\"Took:\"+str( (t1-t0)/60. )+ \" min.\")\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# REsults with standardized-my-theory-values version:\n",
    "'''\n",
    "Optimization terminated successfully.    (Exit mode 0)\n",
    "            Current function value: 6.39496039618e-07\n",
    "            Iterations: 9\n",
    "            Function evaluations: 60\n",
    "            Gradient evaluations: 9\n",
    "Took:0.344144268831 min.\n",
    "Out[12]:\n",
    "     fun: 6.3949603961751021e-07\n",
    "     jac: array([-0.00169004, -0.00146682,  0.001104  ,  0.00482058,  0.        ])\n",
    " message: 'Optimization terminated successfully.'\n",
    "    nfev: 60\n",
    "     nit: 9\n",
    "    njev: 9\n",
    "  status: 0\n",
    " success: True\n",
    "       x: array([ 1.35001977, -0.58735602, -0.09799538,  0.97669182])\n",
    "'''            \n",
    "#               [ 1.3334765 , -0.58013206,  0.034585  ,  1.03876482])\n",
    "\n",
    "# NEW:\n",
    "#                 1.33501652, -0.5832553 , -0.31480513,  1.03965954\n",
    "\n",
    "#TRUE:\n",
    "# \n",
    "#    mystery_theta = {'alpha':1.5, \n",
    "#                 'beta':-0.5,\n",
    "#                 'mu':0.5,              # So note that mu is not correct. \n",
    "#                 'sigma':1.2}           # Still not gotten great. \n",
    "    \n",
    "    \n",
    "    \n",
    "# =======================================================\n",
    "# REsults with data *not* standardized version:\n",
    "'''\n",
    "Optimization terminated successfully.    (Exit mode 0)\n",
    "            Current function value: 5.65339794279e-08\n",
    "            Iterations: 10\n",
    "            Function evaluations: 63\n",
    "            Gradient evaluations: 10\n",
    "Took:0.227895700932 min.\n",
    "Out[20]:\n",
    "     fun: 5.6533979427881372e-08\n",
    "     jac: array([ -8.66875749e-04,   3.26018435e-04,  -6.81478371e-05,\n",
    "        -9.61340689e-04,   0.00000000e+00])\n",
    " message: 'Optimization terminated successfully.'\n",
    "    nfev: 63\n",
    "     nit: 10\n",
    "    njev: 10\n",
    "  status: 0\n",
    " success: True\n",
    "       x: array([ 1.35006237, -0.58585084, -0.14527886,  0.9652678 ])\n",
    "'''\n",
    "    \n",
    "    \n",
    "# REsults with data standardized by mean, stdev, version:\n",
    "'''\n",
    "Optimization terminated successfully.    (Exit mode 0)\n",
    "            Current function value: 9.45908866437e-09\n",
    "            Iterations: 18\n",
    "            Function evaluations: 112\n",
    "            Gradient evaluations: 18\n",
    "Took:0.31707701683 min.\n",
    "Out[16]:\n",
    "     fun: 9.4590886643706467e-09\n",
    "     jac: array([  4.14401579e-04,  -5.15464206e-05,   1.52895583e-04,\n",
    "         1.10592089e-04,   0.00000000e+00])\n",
    " message: 'Optimization terminated successfully.'\n",
    "    nfev: 112\n",
    "     nit: 18\n",
    "    njev: 18\n",
    "  status: 0\n",
    " success: True\n",
    "       x: array([ 1.35011253, -0.5861334 ,  0.06476111,  0.21112074])\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "theta_minus_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find the quandratic value:\n",
    "\n",
    "val = np.dot(theta_minus_theta, W0)\n",
    "val = np.dot(val, theta_minus_theta)\n",
    "val\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix A: Next Extension\n",
    "\n",
    "Extensions:\n",
    "\n",
    "- Weighted empirical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
